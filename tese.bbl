\providecommand{\abntreprintinfo}[1]{%
 \citeonline{#1}}
\setlength{\labelsep}{0pt}\begin{thebibliography}{}
\providecommand{\abntrefinfo}[3]{}
\providecommand{\abntbstabout}[1]{}
\abntbstabout{v-1.9.7 }

\bibitem[Branscombe 2018]{branscombe2018microsoft}
\abntrefinfo{Branscombe}{BRANSCOMBE}{2018}
{BRANSCOMBE, M. \emph{How Microsoft is making its most sensitive HoloLens depth
  sensor yet}. 2018.
\url{https://www.zdnet.com/article/how-microsoft-is-making-its-most-sensitive-hololens-depth-sensor-yet/}.}

\bibitem[Castellano, Terreran e Ghidoni 2023]{castellano2023performance}
\abntrefinfo{Castellano, Terreran e Ghidoni}{CASTELLANO; TERRERAN;
  GHIDONI}{2023}
{CASTELLANO, R.; TERRERAN, M.; GHIDONI, S. Performance evaluation of depth
  completion neural networks for various rgb-d camera technologies in indoor
  scenarios. In:  SPRINGER. \emph{International Conference of the Italian
  Association for Artificial Intelligence}. [S.l.], 2023. p. 351--364.}

\bibitem[Dourado e Pedrino 2020]{dourado2020multi}
\abntrefinfo{Dourado e Pedrino}{DOURADO; PEDRINO}{2020}
{DOURADO, A. M.~B.; PEDRINO, E.~C. Multi-objective cartesian genetic
  programming optimization of morphological filters in navigation systems for
  visually impaired people.
\emph{Applied Soft Computing}, Elsevier, v.~89, p. 106130, 2020.}

\bibitem[Du et al. 2020]{du2020depthlab}
\abntrefinfo{Du et al.}{DU et al.}{2020}
{DU, R. et al. Depthlab: Real-time 3d interaction with depth maps for mobile
  augmented reality. In:  \emph{Proceedings of the 33rd Annual ACM Symposium on
  User Interface Software and Technology}. [S.l.: s.n.], 2020. p. 829--843.}

\bibitem[Eigen, Puhrsch e Fergus 2014]{eigen2014depth}
\abntrefinfo{Eigen, Puhrsch e Fergus}{EIGEN; PUHRSCH; FERGUS}{2014}
{EIGEN, D.; PUHRSCH, C.; FERGUS, R. Depth map prediction from a single image
  using a multi-scale deep network.
\emph{Advances in neural information processing systems}, v.~27, 2014.}

\bibitem[Elharrouss et al. 2020]{elharrouss2020image}
\abntrefinfo{Elharrouss et al.}{ELHARROUSS et al.}{2020}
{ELHARROUSS, O. et al. Image inpainting: A review.
\emph{Neural Processing Letters}, Springer, v.~51, p. 2007--2028, 2020.}

\bibitem[Farkhani et al. 2019]{farkhani2019sparse}
\abntrefinfo{Farkhani et al.}{FARKHANI et al.}{2019}
{FARKHANI, S. et al. Sparse-to-dense depth completion in precision farming. In:
   \emph{Proceedings of the 3rd International Conference on Vision, Image and
  Signal Processing}. [S.l.: s.n.], 2019. p.~1--5.}

\bibitem[Hansard et al. 2012]{hansard2012time}
\abntrefinfo{Hansard et al.}{HANSARD et al.}{2012}
{HANSARD, M. et al. \emph{Time-of-flight cameras: principles, methods and
  applications}. [S.l.]: Springer Science \& Business Media, 2012.}

\bibitem[Hu et al. 2012]{hu2012robust}
\abntrefinfo{Hu et al.}{HU et al.}{2012}
{HU, G. et al. A robust rgb-d slam algorithm. In:  IEEE. \emph{2012 IEEE/RSJ
  International Conference on Intelligent Robots and Systems}. [S.l.], 2012. p.
  1714--1719.}

\bibitem[Hu et al. 2022]{hu2022deep}
\abntrefinfo{Hu et al.}{HU et al.}{2022}
{HU, J. et al. Deep depth completion from extremely sparse data: A survey.
\emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, IEEE,
  v.~45, n.~7, p. 8244--8264, 2022.}

\bibitem[Jaritz et al. 2018]{jaritz2018sparse}
\abntrefinfo{Jaritz et al.}{JARITZ et al.}{2018}
{JARITZ, M. et al. Sparse and dense data with cnns: Depth completion and
  semantic segmentation. In:  IEEE. \emph{2018 International Conference on 3D
  Vision (3DV)}. [S.l.], 2018. p. 52--60.}

\bibitem[Lasinger et al. 2019]{lasinger2019towards}
\abntrefinfo{Lasinger et al.}{LASINGER et al.}{2019}
{LASINGER, K. et al. Towards robust monocular depth estimation: Mixing datasets
  for zero-shot cross-dataset transfer.
\emph{arXiv preprint arXiv:1907.01341}, 2019.}

\bibitem[Ma et al. 2019]{ma2019sparse}
\abntrefinfo{Ma et al.}{MA et al.}{2019}
{MA, F. et al. Sparse depth sensing for resource-constrained robots.
\emph{The International Journal of Robotics Research}, SAGE Publications Sage
  UK: London, England, v.~38, n.~8, p. 935--980, 2019.}

\bibitem[Padhy et al. 2023]{padhy2023monocular}
\abntrefinfo{Padhy et al.}{PADHY et al.}{2023}
{PADHY, R.~P. et al. Monocular vision-aided depth measurement from rgb images
  for autonomous uav navigation.
\emph{ACM Transactions on Multimedia Computing, Communications and
  Applications}, ACM New York, NY, v.~20, n.~2, p. 1--22, 2023.}

\bibitem[Park, Lee e Ko 2021]{park2021enabling}
\abntrefinfo{Park, Lee e Ko}{PARK; LEE; KO}{2021}
{PARK, H.; LEE, Y.; KO, J. Enabling real-time sign language translation on
  mobile platforms with on-board depth cameras.
\emph{Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous
  Technologies}, ACM New York, NY, USA, v.~5, n.~2, p. 1--30, 2021.}

\bibitem[Roberts et al. 2021]{roberts2021hypersim}
\abntrefinfo{Roberts et al.}{ROBERTS et al.}{2021}
{ROBERTS, M. et al. Hypersim: A photorealistic synthetic dataset for holistic
  indoor scene understanding. In:  \emph{Proceedings of the IEEE/CVF
  international conference on computer vision}. [S.l.: s.n.], 2021. p.
  10912--10922.}

\bibitem[See, Sasing e Advincula 2022]{see2022smartphone}
\abntrefinfo{See, Sasing e Advincula}{SEE; SASING; ADVINCULA}{2022}
{SEE, A.~R.; SASING, B.~G.; ADVINCULA, W.~D. A smartphone-based mobility
  assistant using depth imaging for visually impaired and blind.
\emph{Applied Sciences}, MDPI, v.~12, n.~6, p.~2802, 2022.}

\bibitem[Song et al. 2021]{song2021self}
\abntrefinfo{Song et al.}{SONG et al.}{2021}
{SONG, Z. et al. Self-supervised depth completion from direct visual-lidar
  odometry in autonomous driving.
\emph{IEEE Transactions on Intelligent Transportation Systems}, IEEE, v.~23,
  n.~8, p. 11654--11665, 2021.}

\bibitem[Suvorov et al. 2022]{suvorov2022resolution}
\abntrefinfo{Suvorov et al.}{SUVOROV et al.}{2022}
{SUVOROV, R. et al. Resolution-robust large mask inpainting with fourier
  convolutions. In:  \emph{Proceedings of the IEEE/CVF winter conference on
  applications of computer vision}. [S.l.: s.n.], 2022. p. 2149--2159.}

\bibitem[Xie et al. 2021]{xie2021ultradepth}
\abntrefinfo{Xie et al.}{XIE et al.}{2021}
{XIE, Z. et al. Ultradepth: Exposing high-resolution texture from depth
  cameras. In:  \emph{Proceedings of the 19th ACM Conference on Embedded
  Networked Sensor Systems}. [S.l.: s.n.], 2021. p. 302--315.}

\bibitem[Zhang e Funkhouser 2018]{zhang2018deep}
\abntrefinfo{Zhang e Funkhouser}{ZHANG; FUNKHOUSER}{2018}
{ZHANG, Y.; FUNKHOUSER, T. Deep depth completion of a single rgb-d image. In:
  \emph{Proceedings of the IEEE conference on computer vision and pattern
  recognition}. [S.l.: s.n.], 2018. p. 175--185.}

\bibitem[Zhang et al. 2022]{zhang2022indepth}
\abntrefinfo{Zhang et al.}{ZHANG et al.}{2022}
{ZHANG, Y. et al. Indepth: Real-time depth inpainting for mobile augmented
  reality.
\emph{Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous
  Technologies}, ACM New York, NY, USA, v.~6, n.~1, p. 1--25, 2022.}

\bibitem[Zhou, Kr{\"a}henb{\"u}hl e Koltun 2019]{zhou2019does}
\abntrefinfo{Zhou, Kr{\"a}henb{\"u}hl e Koltun}{ZHOU; KR{\"A}HENB{\"U}HL;
  KOLTUN}{2019}
{ZHOU, B.; KR{\"A}HENB{\"U}HL, P.; KOLTUN, V. Does computer vision matter for
  action?
\emph{Science Robotics}, American Association for the Advancement of Science,
  v.~4, n.~30, p. eaaw6661, 2019.}

\bibitem[Zollh{\"o}fer 2019]{zollhofer2019commodity}
\abntrefinfo{Zollh{\"o}fer}{ZOLLH{\"O}FER}{2019}
{ZOLLH{\"O}FER, M. Commodity rgb-d sensors: Data acquisition.
\emph{RGB-D image analysis and processing}, Springer, p. 3--13, 2019.}

\end{thebibliography}
